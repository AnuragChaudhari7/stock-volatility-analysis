{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165f82d1-c9f0-4ff8-a2c2-6e906bb4aabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Forecasting Realised Volatility with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039e519b-f88d-47a4-ae8f-1538d32ac4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import functions and libraries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475bde85-750d-4af4-b4ee-973d9f9127a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"QuantVolatility\").getOrCreate()\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"../data/nifty100_combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6961b5f-35c8-4f4d-a354-99563e2202dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the table\n",
    "#df = spark.table(\"workspace.default.nifty_100_combined_data\")\n",
    "#display(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0194a58-95e6-40b9-aa5d-3a88ee23ab72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b79fc46-e26c-4b3c-9e43-7eda9a11043d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define windows\n",
    "window = Window.partitionBy(\"ticker\").orderBy(\"date\")\n",
    "rolling_1h_back_window = window.rowsBetween(-11, 0)\n",
    "rolling_1h_forward_window = window.rowsBetween(1, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4314717-b900-4235-96f1-6bd597919136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Split dataframe modification\n",
    "# Compute 1 tick lagged close\n",
    "df = df. \\\n",
    "        withColumn(\"lag_close\", sf.lag(\"close\").over(window)\n",
    ")\n",
    "\n",
    "# Compute the log return on positive closes\n",
    "df = df. \\\n",
    "        withColumn(\n",
    "            \"log_return\", \n",
    "            sf.when(\n",
    "                sf.col(\"lag_close\").isNotNull() & (sf.col(\"lag_close\") != 0),\n",
    "                sf.log(sf.col(\"close\") / sf.col(\"lag_close\"))\n",
    "            ).otherwise(sf.lit(None))\n",
    ")\n",
    "\n",
    "# Compute the abs return \n",
    "df = df. \\\n",
    "        withColumn(\"abs_return\", sf.abs(sf.col(\"log_return\"))\n",
    ")\n",
    "\n",
    "# Compute high low diff\n",
    "df = df. \\\n",
    "        withColumn(\"hl_range\", sf.col(\"high\") - sf.col(\"low\")\n",
    ")\n",
    "\n",
    "# Compute 1 tick lagged volume\n",
    "df = df. \\\n",
    "        withColumn(\"lag_volume\", sf.lag(\"volume\").over(window)\n",
    ")\n",
    "\n",
    "# Compute the log vol change\n",
    "df = df. \\\n",
    "        withColumn(\n",
    "            \"log_volume_change\", \n",
    "            sf.when(\n",
    "                sf.col(\"lag_volume\").isNotNull() & (sf.col(\"lag_volume\") != 0),\n",
    "                sf.log(sf.col(\"volume\") / sf.col(\"lag_volume\"))\n",
    "            ).otherwise(sf.lit(None))\n",
    ")\n",
    "\n",
    "#Extract the hour from timestamp for ease\n",
    "df = df. \\\n",
    "        withColumn(\n",
    "            \"hour\", \n",
    "            sf.hour(sf.col(\"date\"))\n",
    ")\n",
    "\n",
    "# Compute 1 hour lagged realised vol\n",
    "df = df. \\\n",
    "        withColumn(\"window_count\", sf.count(\"log_return\").over(rolling_1h_back_window))\n",
    "\n",
    "\n",
    "df = df. \\\n",
    "        withColumn(\"realised_rolling_vol_backward\", \n",
    "                                sf.when(\n",
    "                                    sf.col(\"window_count\") == 12, \n",
    "                                    sf.sqrt(sf.sum(sf.pow(sf.col(\"log_return\"), 2)).over(rolling_1h_back_window))\n",
    "                                )\n",
    "                                .otherwise(sf.lit(None))\n",
    "                    )\n",
    "\n",
    "df = df. \\\n",
    "        drop(\"window_count\")\n",
    "\n",
    "# Compute 1 hour forward realised vol\n",
    "df = df. \\\n",
    "        withColumn(\"window_count\", sf.count(\"log_return\").over(rolling_1h_forward_window))\n",
    "\n",
    "df = df. \\\n",
    "        withColumn(\"realised_rolling_vol_forward\", \n",
    "                                sf.when(\n",
    "                                    sf.col(\"window_count\") == 12, \n",
    "                                    sf.sqrt(sf.sum(sf.pow(sf.col(\"log_return\"), 2)).over(rolling_1h_forward_window))\n",
    "                                )\n",
    "                                .otherwise(sf.lit(None))\n",
    "                    )\n",
    "\n",
    "df = df. \\\n",
    "        drop(\"window_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843be500-f352-4b98-b5ed-75d4b89f4d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f91ba2a-ce55-4f7b-bf8f-e591cbd63e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4b743c-f44e-4c85-838c-0b9541976e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the null values from the dataset (part of cleaning)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec5012d-53d8-4903-8b90-fdb1ab114ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(df)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a28c2d0a-d0d2-4c20-8f5f-edcb9018c81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Save Data as Spark Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a179703-e0a6-41db-bb28-9edc77ec1ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# N.B/ for improved perforamnce:\n",
    "#df.write.partitionBy(\"ticker\").mode(\"overwrite\").saveAsTable(\"cleaned_volatility_data\")\n",
    "#df.write.mode(\"overwrite\").saveAsTable(\"cleaned_volatility_data\")\n",
    "\n",
    "df.write.partitionBy(\"ticker\").mode(\"overwrite\").parquet(\"../data/cleaned_volatility_data.par\")\n",
    "\n",
    "clean_df = spark.read.parquet(\"../data/cleaned_volatility_data.par\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8983bae-0253-4718-aecf-57e0ab9e6fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8909169e-736e-4b8d-9345-cc5cf7963962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a660f1da-9477-4242-8780-b6cd4f1f6d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import SparkML tools\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe94f603-b524-4135-be70-ea730d62dda3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b03072d4-fc80-4556-89bf-f444a5849c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Encode the ticker column for usage in Model\n",
    "indexer = StringIndexer(\n",
    "            inputCol=\"ticker\",\n",
    "            outputCol=\"ticker_index\"\n",
    ")\n",
    "\n",
    "clean_df = indexer.fit(clean_df).transform(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53dbd1e0-ae52-4870-887b-30deb5c6e337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compact features and target into smaller df\n",
    "features = [\n",
    "    \"abs_return\",\n",
    "    \"hl_range\",\n",
    "    \"log_volume_change\",\n",
    "    \"hour\",\n",
    "    \"realised_rolling_vol_backward\",\n",
    "    \"ticker_index\"\n",
    "]\n",
    "    \n",
    "assembler = VectorAssembler(\n",
    "                inputCols=features,\n",
    "                outputCol=\"features\"\n",
    ")\n",
    "\n",
    "model_df = assembler.transform(clean_df).select(\"features\", \"realised_rolling_vol_forward\", \"ticker\").dropna()\n",
    "model_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9893873b-3fb9-4a62-aca1-64ce8cbc48c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and test data\n",
    "#TODO: should data be split in order\n",
    "\n",
    "tickers = [row[\"ticker\"] for row in clean_df.select(\"ticker\").distinct().collect()] #only use collect if small no. of records (all appear in driver)\n",
    "\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "# filter each ticker and split its rows randomly into train and test\n",
    "for t in tickers:\n",
    "    df_t = model_df.filter(sf.col(\"ticker\") == t)\n",
    "    train_t, test_t = df_t.randomSplit([0.8,0.2], seed=69)\n",
    "    train_parts.append(train_t)\n",
    "    test_parts.append(test_t)\n",
    "\n",
    "# now combine the different parts into a unified df\n",
    "train_df = reduce(lambda a, b: a.union(b), train_parts)\n",
    "test_df = reduce(lambda a, b: a.union(b), test_parts)\n",
    "\n",
    "#now drop ticker from both\n",
    "train_df.drop(sf.col(\"ticker\"))\n",
    "test_df.drop(sf.col(\"ticker\"))\n",
    "\n",
    "train_df.show()\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb04ce95-e1d1-4fb9-bc86-e704dce4f32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup and train the RF model\n",
    "rf = RandomForestRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"realised_rolling_vol_forward\",\n",
    "        numTrees=100,\n",
    "        maxDepth=10,\n",
    "        maxBins=99,\n",
    "        seed=69\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1939392a-80d9-4c07-8daf-5236093eaaca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = rf_model.transform(test_df)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d25438-45bb-4b18-ae42-d340040f65be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "                labelCol=\"realised_rolling_vol_forward\",\n",
    "                predictionCol=\"prediction\",\n",
    "                metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25d5abbe-e0be-4177-8bbb-65c4b34d0440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator_r2 = RegressionEvaluator(\n",
    "                labelCol=\"realised_rolling_vol_forward\",\n",
    "                predictionCol=\"prediction\",\n",
    "                metricName=\"r2\"\n",
    ")\n",
    "\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"R2: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "forecasting-volatility-ml",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
