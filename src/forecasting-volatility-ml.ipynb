{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165f82d1-c9f0-4ff8-a2c2-6e906bb4aabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Forecasting Realised Volatility with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039e519b-f88d-47a4-ae8f-1538d32ac4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import functions and libraries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6961b5f-35c8-4f4d-a354-99563e2202dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the table\n",
    "df = spark.table(\"workspace.default.nifty_100_combined_data\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0194a58-95e6-40b9-aa5d-3a88ee23ab72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b79fc46-e26c-4b3c-9e43-7eda9a11043d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define windows\n",
    "window = Window.partitionBy(\"ticker\").orderBy(\"date\")\n",
    "rolling_1h_back_window = window.rowsBetween(-11, 0)\n",
    "rolling_1h_forward_window = window.rowsBetween(1, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4314717-b900-4235-96f1-6bd597919136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute 1 tick lagged close\n",
    "df = df. \\\n",
    "        withColumn(\"lag_close\", sf.lag(\"close\").over(window)\n",
    ")\n",
    "\n",
    "# Compute the log return on positive closes\n",
    "df = df. \\\n",
    "        withColumn(\n",
    "            \"log_return\", \n",
    "            sf.when(\n",
    "                sf.col(\"lag_close\").isNotNull() & (sf.col(\"lag_close\") != 0),\n",
    "                sf.log(sf.col(\"close\") / sf.col(\"lag_close\"))\n",
    "            ).otherwise(sf.lit(None))\n",
    ")\n",
    "\n",
    "# Compute the abs return \n",
    "df = df. \\\n",
    "        withColumn(\"abs_return\", sf.abs(sf.col(\"log_return\"))\n",
    ")\n",
    "\n",
    "# Compute high low diff\n",
    "df = df. \\\n",
    "        withColumn(\"hl_range\", sf.col(\"high\") - sf.col(\"low\")\n",
    ")\n",
    "\n",
    "# Compute 1 tick lagged volume\n",
    "df = df. \\\n",
    "        withColumn(\"lag_volume\", sf.lag(\"volume\").over(window)\n",
    ")\n",
    "\n",
    "# Compute the log vol change\n",
    "df = df. \\\n",
    "        withColumn(\n",
    "            \"log_volume_change\", \n",
    "            sf.when(\n",
    "                sf.col(\"lag_volume\").isNotNull() & (sf.col(\"lag_volume\") != 0),\n",
    "                sf.log(sf.col(\"volume\") / sf.col(\"lag_volume\"))\n",
    "            ).otherwise(sf.lit(None))\n",
    ")\n",
    "\n",
    "#Extract the hour from timestamp for ease\n",
    "df = df. \\\n",
    "        withColumn(\n",
    "            \"hour\", \n",
    "            sf.hour(sf.col(\"date\"))\n",
    ")\n",
    "\n",
    "# Compute 1 hour lagged realised vol\n",
    "df = df. \\\n",
    "        withColumn(\"window_count\", sf.count(\"log_return\").over(rolling_1h_back_window))\n",
    "\n",
    "\n",
    "df = df. \\\n",
    "        withColumn(\"realised_rolling_vol_backward\", \n",
    "                                sf.when(\n",
    "                                    sf.col(\"window_count\") == 12, \n",
    "                                    sf.sqrt(sf.sum(sf.pow(sf.col(\"log_return\"), 2)).over(rolling_1h_back_window))\n",
    "                                )\n",
    "                                .otherwise(sf.lit(None))\n",
    "                    )\n",
    "\n",
    "df = df. \\\n",
    "        drop(\"window_count\")\n",
    "\n",
    "# Compute 1 hour forward realised vol\n",
    "df = df. \\\n",
    "        withColumn(\"window_count\", sf.count(\"log_return\").over(rolling_1h_forward_window))\n",
    "\n",
    "df = df. \\\n",
    "        withColumn(\"realised_rolling_vol_forward\", \n",
    "                                sf.when(\n",
    "                                    sf.col(\"window_count\") == 12, \n",
    "                                    sf.sqrt(sf.sum(sf.pow(sf.col(\"log_return\"), 2)).over(rolling_1h_forward_window))\n",
    "                                )\n",
    "                                .otherwise(sf.lit(None))\n",
    "                    )\n",
    "\n",
    "df = df. \\\n",
    "        drop(\"window_count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843be500-f352-4b98-b5ed-75d4b89f4d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f91ba2a-ce55-4f7b-bf8f-e591cbd63e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4b743c-f44e-4c85-838c-0b9541976e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the null values from the dataset (part of cleaning)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec5012d-53d8-4903-8b90-fdb1ab114ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a28c2d0a-d0d2-4c20-8f5f-edcb9018c81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Save Data as Spark Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a179703-e0a6-41db-bb28-9edc77ec1ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# N.B/ for improved perforamnce:\n",
    "#df.write.partitionBy(\"ticker\").mode(\"overwrite\").saveAsTable(\"cleaned_volatility_data\")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"cleaned_volatility_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8909169e-736e-4b8d-9345-cc5cf7963962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a660f1da-9477-4242-8780-b6cd4f1f6d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "forecasting-volatility-ml",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
